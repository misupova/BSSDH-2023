{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/misupova/BSSDH-2023/blob/main/3_web_scraping_intro_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scraping with Python\n",
        "\n",
        "Web scraping is a method used to extract data from websites. This process involves making HTTP requests to the specific URLs of the websites you are interested in, and then parsing the HTML data that is returned to extract the information you need.\n",
        "\n",
        "In this notebook, we will be using Python to scrape data from several Wikipedia pages. We will guide you through the process of identifying the HTML tags and attributes that contain the data you want, and then writing a Python script to extract this data and save it to a CSV file.\n",
        "\n",
        "## **Your main task is to run cells one by one, fill in input fields, and check whether the script is indeed performing scraping.**\n",
        "\n",
        "As a reminder, basic HTML structure looks like this:\n",
        "\n",
        "![HTML_structure](https://stuyhsdesign.files.wordpress.com/2015/09/basic-structure.png)\n",
        "\n",
        "And HTML tag structure looks like this:\n",
        "\n",
        "![HTML_tag](https://tutorial.techaltum.com/images/element.png)\n",
        "\n",
        "Before we start, we need to import the necessary Python libraries that we will be using for our web scraping script. If you don't have these libraries installed, you can do so by running the command `!pip install library_name`.\n",
        "\n",
        "Here are the libraries we will be using:\n",
        "\n",
        "- `requests`: This library allows us to send HTTP requests.\n",
        "- `BeautifulSoup`: This library is used for parsing the HTML data returned by our requests.\n",
        "- `pandas`: We will use this library to handle our data and save it to a CSV file.\n",
        "\n",
        "Let's go ahead and import these libraries."
      ],
      "metadata": {
        "id": "LjBRUcS6Etvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Install libraries\n",
        "!pip install BeautifulSoup"
      ],
      "metadata": {
        "id": "D_2t4RePExdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from ipywidgets import widgets\n",
        "from ipywidgets import Layout\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "ebeJZpnAE9_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have imported our libraries, we can move on to the next step: identifying the HTML tags and attributes that contain the data we want to scrape. In the following sections, you will be provided with input fields where you can specify these tags and attributes.\n"
      ],
      "metadata": {
        "id": "Gw0a3hZWFF5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inputs\n",
        "\n",
        "In this section, we will specify the input parameters for our web scraping script. These include the URLs we want to scrape data from, the HTML tags that contain the data we're interested in, and the name of the CSV file we want to save our data to.\n",
        "\n",
        "You have interactive input fields for these parameters. These fields will allow you to easily change the input parameters without having to modify the code.\n",
        "\n",
        "To explain further, in HTML, an attribute is a property used to provide additional information about an HTML element. Attributes are included in the opening tag and are made up of a pair: the attribute name and the attribute value. The general form is attribute_name=\"attribute_value\". For example, in the tag `<img src=\"image.jpg\" alt=\"My Image\">`, src and alt are attributes, with image.jpg and My Image as their respective values.\n",
        "\n",
        "In the context of web scraping, attributes can be very useful for narrowing down the elements that you want to extract data from.\n",
        "\n",
        "We have the following input fields:\n",
        "\n",
        "- **URL List**: A list of URLs that you want to scrape data from.\n",
        "- **Title Tag**: The HTML tag that contains the title of the page.\n",
        "- **Intro Tag**: The HTML tag that contains the introduction or summary of the page.\n",
        "- **Intro Paragraphs**: The number of non-empty introduction paragraphs to scrape from the page.\n",
        "- **Image Tags**: The HTML tag used for images.\n",
        "- **Link Tags**: The HTML tag used for hyperlinks.\n",
        "- **CSV File Name**: The name of the CSV file to save the scraped data to.\n",
        "- **Table of Contents Tag**: The HTML tag that contains the table of contents.\n",
        "- **Table of Contents Attribute**: The HTML attribute used to identify the table of contents tag.\n",
        "- **Table of Contents Attribute Value**: The value of the HTML attribute used to identify the table of contents tag."
      ],
      "metadata": {
        "id": "cZSjuttmFWcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# URL list input\n",
        "url_list_input = widgets.Textarea(\n",
        "    placeholder='Enter URLs, separated by commas',\n",
        "    description='URL List:',\n",
        "    layout=Layout(width='auto', height='100px')\n",
        ")\n",
        "url_list_input.style.description_width = 'initial'\n",
        "\n",
        "# Title tag input\n",
        "title_tag_input = widgets.Text(\n",
        "    placeholder='Enter HTML tag for title',\n",
        "    description='Title Tag:',\n",
        "    layout=Layout(width='auto')\n",
        ")\n",
        "title_tag_input.style.description_width = 'initial'\n",
        "\n",
        "# Intro tag input\n",
        "intro_tag_input = widgets.Text(\n",
        "    placeholder='Enter HTML tag for intro',\n",
        "    description='Intro Tag:',\n",
        "    layout=Layout(width='auto')\n",
        ")\n",
        "intro_tag_input.style.description_width = 'initial'\n",
        "\n",
        "# Number of intro paragraphs\n",
        "intro_paragraphs_input = widgets.IntSlider(\n",
        "    min=0,\n",
        "    max=10,\n",
        "    step=1,\n",
        "    description='Intro Paragraphs:',\n",
        ")\n",
        "intro_paragraphs_input.style.description_width = 'initial'\n",
        "\n",
        "# Image tags input\n",
        "image_tags_input = widgets.Text(\n",
        "    placeholder='Enter HTML tag for images',\n",
        "    description='Image Tags:',\n",
        "    layout=Layout(width='auto')\n",
        ")\n",
        "image_tags_input.style.description_width = 'initial'\n",
        "\n",
        "# Link tags input\n",
        "link_tags_input = widgets.Text(\n",
        "    placeholder='Enter HTML tag for links',\n",
        "    description='Link Tags:',\n",
        "    layout=Layout(width='auto')\n",
        ")\n",
        "link_tags_input.style.description_width = 'initial'\n",
        "\n",
        "# Table of Contents Tag input\n",
        "toc_tag_input = widgets.Text(\n",
        "    placeholder='Enter HTML tag for table of contents',\n",
        "    description='ToC Tag:',\n",
        "    layout=Layout(width='auto')\n",
        ")\n",
        "toc_tag_input.style.description_width = 'initial'\n",
        "\n",
        "# Table of Contents Attribute input\n",
        "toc_attribute_input = widgets.Text(\n",
        "    placeholder='Enter attribute name for ToC tag',\n",
        "    description='ToC Attribute Name:',\n",
        "    layout=Layout(width='auto')\n",
        ")\n",
        "toc_attribute_input.style.description_width = 'initial'\n",
        "\n",
        "# Table of Contents Attribute Value input\n",
        "toc_attribute_value_input = widgets.Text(\n",
        "    placeholder='Enter attribute value for ToC tag',\n",
        "    description='ToC Attribute Value:',\n",
        "    layout=Layout(width='auto')\n",
        ")\n",
        "toc_attribute_value_input.style.description_width = 'initial'\n",
        "\n",
        "# CSV file name input\n",
        "csv_file_name_input = widgets.Text(\n",
        "    value='scraped_data.csv',\n",
        "    placeholder='Enter name of the CSV file',\n",
        "    description='CSV File Name:',\n",
        "    layout=Layout(width='auto')\n",
        ")\n",
        "csv_file_name_input.style.description_width = 'initial'\n",
        "\n",
        "# Display the widgets\n",
        "display(url_list_input, title_tag_input, intro_tag_input, intro_paragraphs_input,\n",
        "        image_tags_input, link_tags_input,\n",
        "        toc_tag_input, toc_attribute_input, toc_attribute_value_input,\n",
        "        csv_file_name_input)"
      ],
      "metadata": {
        "id": "zOFs0zDwFBni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can fill in the input fields above with your desired parameters. Once you're done, we can move on to the next section where we'll define the functions for our web scraping script."
      ],
      "metadata": {
        "id": "2eNNHcpaxWT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function Definitions\n",
        "\n",
        "In this section, we will define several Python functions that will be used in our web scraping script.\n",
        "\n",
        "These functions will be responsible for:\n",
        "\n",
        "1. Sending a GET request to a URL and returning the response.\n",
        "2. Parsing the HTML content of the response using BeautifulSoup.\n",
        "3. Extracting the required data from the parsed HTML.\n",
        "4. Writing the scraped data to a CSV file.\n",
        "\n",
        "Let's go ahead and define these functions.\n"
      ],
      "metadata": {
        "id": "NTtwbOj4HBBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to send a GET request to a URL\n",
        "def fetch_url(url):\n",
        "    response = requests.get(url)\n",
        "    return response\n",
        "\n",
        "# Function to parse HTML content\n",
        "def parse_html(content):\n",
        "    soup = BeautifulSoup(content, 'html.parser')\n",
        "    return soup\n",
        "\n",
        "# Function to extract data from parsed HTML\n",
        "def extract_data(url, soup, title_tag, intro_tag, intro_paragraphs, image_tags, link_tags,\n",
        "                 toc_tag, toc_attribute, toc_attribute_value):\n",
        "    data = {}\n",
        "\n",
        "    # Extract URL first\n",
        "    data['url'] = url\n",
        "\n",
        "    # Extract title\n",
        "    title = soup.find(title_tag)\n",
        "    if title:\n",
        "        data['title'] = title.text.strip()\n",
        "\n",
        "    # Extract table of contents\n",
        "    toc = soup.find(toc_tag, attrs={toc_attribute: toc_attribute_value})\n",
        "    if toc:\n",
        "        data['toc'] = toc.text.strip()\n",
        "\n",
        "    # Extract intro paragraphs\n",
        "    intro_paras = soup.find_all(intro_tag)\n",
        "    # Filter out empty paragraphs or paragraphs only containing whitespace\n",
        "    intro_paras = [para for para in intro_paras if para.text.strip() != '']\n",
        "    intro_paras = intro_paras[:intro_paragraphs] if len(intro_paras) > intro_paragraphs else intro_paras\n",
        "    if intro_paras:\n",
        "        data['intro'] = ' '.join([para.text.strip() for para in intro_paras])\n",
        "\n",
        "    # Extract image URLs\n",
        "    image_urls = []\n",
        "    for tag in image_tags:\n",
        "        images = soup.find_all(tag)\n",
        "        if images:\n",
        "            image_urls.extend([img.get('src') for img in images])\n",
        "    data['image_urls'] = image_urls\n",
        "\n",
        "    # Extract links\n",
        "    links = []\n",
        "    for tag in link_tags:\n",
        "        anchors = soup.find_all(tag)\n",
        "        if anchors:\n",
        "            links.extend([link.get('href') for link in anchors])\n",
        "    data['links'] = links\n",
        "\n",
        "    return data\n",
        "\n",
        "# Function to write data to CSV\n",
        "def write_to_csv(data, file_name):\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(file_name, index=False)\n"
      ],
      "metadata": {
        "id": "giInlb95FZPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scraping\n",
        "\n",
        "Now that we've defined our input parameters and functions, we can move on to the main part of our script: the web scraping.\n",
        "\n",
        "In this section, we will use the functions defined above to:\n",
        "\n",
        "1. Send a GET request to each URL in our URL list.\n",
        "2. Parse the HTML content of each response.\n",
        "3. Extract the required data from the parsed HTML.\n",
        "4. Save the scraped data to a CSV file.\n",
        "\n",
        "Let's start the web scraping.\n"
      ],
      "metadata": {
        "id": "QjgDAp19HRIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the string input values into correct data types\n",
        "url_list = url_list_input.value.split(',')\n",
        "title_tag = title_tag_input.value\n",
        "intro_tag = intro_tag_input.value\n",
        "intro_paragraphs = intro_paragraphs_input.value\n",
        "image_tags = image_tags_input.value.split(',')\n",
        "link_tags = link_tags_input.value.split(',')\n",
        "csv_file_name = csv_file_name_input.value\n",
        "toc_tag = toc_tag_input.value\n",
        "toc_attribute = toc_attribute_input.value\n",
        "toc_attribute_value = toc_attribute_value_input.value\n",
        "\n",
        "# Initialize an empty list to store the scraped data\n",
        "data_list = []\n",
        "\n",
        "# Iterate over each URL in the URL list\n",
        "for url in url_list:\n",
        "    # Fetch the URL\n",
        "    response = fetch_url(url)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content of the response\n",
        "        soup = parse_html(response.content)\n",
        "        # Extract the required data\n",
        "        data = extract_data(url, soup, title_tag, intro_tag, intro_paragraphs, image_tags, link_tags, toc_tag, toc_attribute, toc_attribute_value)\n",
        "        # Add the data to our data list\n",
        "        data_list.append(data)\n",
        "\n",
        "# Convert the data list into a pandas DataFrame\n",
        "df = pd.DataFrame(data_list)\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df)\n",
        "\n",
        "# Write the DataFrame to a CSV file\n",
        "df.to_csv(csv_file_name, index=False)\n",
        "\n",
        "print(f\"Data saved to {csv_file_name}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "SqcCN6e1Hk2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The web scraping process is now complete! The data has been written to a CSV file named according to the input you provided.\n",
        "\n",
        "In the next section, we'll discuss potential next steps and extensions to this web scraping script.\n"
      ],
      "metadata": {
        "id": "SElQbjByHmwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion and Next Steps\n",
        "\n",
        "Congratulations! You've successfully scraped a web page using Python.\n",
        "\n",
        "In this notebook, we covered:\n",
        "\n",
        "- The process of making HTTP requests to retrieve data from a website.\n",
        "- How to parse the HTML response and identify the tags and attributes that contain the information we're interested in.\n",
        "- How to extract this information and save it to a CSV file.\n",
        "\n",
        "Remember, this is just the beginning. There's so much more you can do with web scraping. Here are a few ideas for next steps:\n",
        "\n",
        "- Explore other HTML tags and attributes. The web is filled with all sorts of interesting data, and you never know what you might find!\n",
        "- Improve the output format - clean the data, sort it by tags or attributes, save the data in different formats\n",
        "- Try scraping different websites. Each website is different, and you'll need to adjust your scraping strategy to match the structure of each site.\n",
        "- Learn about handling more complex scenarios, like JavaScript-heavy sites, dealing with cookies and sessions, and handling different data formats like JSON and XML.\n",
        "- Always remember to respect the terms of service of the sites you're scraping. Some sites may have rules against scraping, or require you to identify your bot in a specific way.\n",
        "\n"
      ],
      "metadata": {
        "id": "t-2Uk3E3IRcz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional: Data Cleaning\n",
        "\n",
        "After extracting data from the web, it's common to find that the data is not in the exact format we want. It might include extra spaces, irrelevant characters, HTML tags, or other elements that we don't need for our analysis or data processing. This is where data cleaning comes in.\n",
        "\n",
        "Data cleaning is the process of identifying and correcting (or removing) errors in the dataset. It is an important and often necessary step in the data preprocessing pipeline, especially when dealing with unstructured data like web scraped content.\n",
        "\n",
        "In the context of our web scraping task, data cleaning can involve processes such as:\n",
        "\n",
        "    Removing HTML tags from the scraped text.\n",
        "    Replacing multiple spaces with a single space.\n",
        "    Removing leading and trailing whitespaces.\n",
        "    Standardizing or removing special characters.\n",
        "    Ensuring links are complete and well-formed.\n",
        "\n",
        "Keep in mind that the extent of data cleaning required can vary greatly depending on the data and the specific task or analysis you're planning to do. Therefore, this step is optional and customizable based on your needs. Here is a simple function to clean our scraped data."
      ],
      "metadata": {
        "id": "_i4Z2plL0sr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Cleans the input text by removing HTML tags, replacing multiple spaces with a single space, and trimming leading/trailing whitespaces.\"\"\"\n",
        "    # Remove HTML tags\n",
        "    cleaned_text = re.sub('<[^<]+?>', '', text)\n",
        "    # Replace multiple spaces with a single space\n",
        "    cleaned_text = re.sub('\\s+', ' ', cleaned_text)\n",
        "    # Remove leading and trailing whitespaces\n",
        "    cleaned_text = cleaned_text.strip()\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "id": "JHzf8BNSnEPH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}